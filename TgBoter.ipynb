{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcprT4fwcWNV",
        "outputId": "f2a0b6c1-e493-4ac6-dcfd-46c352715559"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/677.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m368.6/677.3 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m677.3/677.3 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install -U aiogram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LUdcj8kPgIS",
        "outputId": "9af388ce-efe7-458f-e371-5414d0a385f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.9/124.9 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install azure-ai-inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5FYhLowda6H"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "OwFEp_H3dmmU"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "import os\n",
        "from aiogram import Bot, Dispatcher, types\n",
        "from aiogram.filters import Command\n",
        "from azure.ai.inference import ChatCompletionsClient\n",
        "from azure.ai.inference.models import SystemMessage, UserMessage\n",
        "from azure.core.credentials import AzureKeyCredential\n",
        "import logging\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGEBXzyNeEz9"
      },
      "outputs": [],
      "source": [
        "# Retrieve environment variables from Google Colab userdata\n",
        "from google.colab import userdata\n",
        "os.environ['GITHUB_TOKEN'] = ''\n",
        "os.environ['TELEGRAM_BOT_TOKEN'] =''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3_PWNGBzf0Ro"
      },
      "outputs": [],
      "source": [
        "dp = Dispatcher()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "dOC9QJY1qP0i"
      },
      "outputs": [],
      "source": [
        "# Context class to store previous responses (basic implementation)\n",
        "class Gpt_context:\n",
        "    def __init__(self):\n",
        "        self.response = \"\"\n",
        "\n",
        "# Create context instance\n",
        "context = Gpt_context()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GoaBucLjf8Sl"
      },
      "outputs": [],
      "source": [
        "# Handler for /start command\n",
        "@dp.message(Command('start'))\n",
        "async def command_start_handler(message: types.Message):\n",
        "    \"\"\"Handles the /start command\"\"\"\n",
        "    await message.answer(\"Hello sir!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3pDE36RwkJUM"
      },
      "outputs": [],
      "source": [
        "# Handler for /help command\n",
        "@dp.message(Command('help'))\n",
        "async def command_help_handler(message: types.Message):\n",
        "    \"\"\"Handles the /help command\"\"\"\n",
        "    help_command = \"\"\"\n",
        "    Helo sir U can chose from -\n",
        "    /start\n",
        "    /help\n",
        "    Send text, audio, or image for processing!\n",
        "    \"\"\"\n",
        "    await message.reply(help_command)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jx8vjlWlBsf"
      },
      "outputs": [],
      "source": [
        "# Handler for text messages\n",
        "@dp.message()\n",
        "async def chatgpt(message: types.Message):\n",
        "    \"\"\"Processes text messages using Azure AI Inference (GitHub model)\"\"\"\n",
        "    endpoint = \"https://models.github.ai/inference\"\n",
        "    model = \"openai/gpt-4.1\"\n",
        "    token = \"\"\n",
        "\n",
        "    client = ChatCompletionsClient(\n",
        "        endpoint=endpoint,\n",
        "        credential=AzureKeyCredential(token),\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response = client.complete(\n",
        "            messages=[\n",
        "                SystemMessage(content=\"You are a helpful assistant.\"),\n",
        "                UserMessage(content=message.text),\n",
        "            ],\n",
        "            temperature=1.0,\n",
        "            top_p=1.0,\n",
        "            model=model\n",
        "        )\n",
        "        value = response.choices[0].message.content\n",
        "        context.response = value  # Store response in context\n",
        "        print(\"Bot Response:\", value)\n",
        "        await message.reply(value)\n",
        "    except Exception as e:\n",
        "        await message.reply(f\"Error: {str(e)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yQI_6y0ii2k"
      },
      "outputs": [],
      "source": [
        "# Main function to start the bot\n",
        "async def main() -> None:\n",
        "    bot = Bot('')\n",
        "    await dp.start_polling(bot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "fdDnkvWgW8sT"
      },
      "outputs": [],
      "source": [
        "# Function to run the bot in Colab's event loop\n",
        "async def run_bot():\n",
        "    try:\n",
        "        await main()\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"Bot stopped manually\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "fOrY6s3wVW4O"
      },
      "outputs": [],
      "source": [
        "#Run the bot using the existing event loop\n",
        "if __name__ == \"__main__\":\n",
        "    logging.basicConfig(level=logging.INFO, stream=sys.stdout)\n",
        "    loop = asyncio.get_event_loop()\n",
        "    if loop.is_running():\n",
        "        # If the loop is already running, schedule the coroutine\n",
        "        asyncio.ensure_future(run_bot())\n",
        "    else:\n",
        "        # If the loop is not running, start it\n",
        "        try:\n",
        "            loop.run_until_complete(run_bot())\n",
        "        finally:\n",
        "            loop.run_until_complete(loop.shutdown_asyncgens())\n",
        "            loop.close()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
